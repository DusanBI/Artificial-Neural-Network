{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.interpolate import griddata\n",
    "import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import  MinMaxScaler\n",
    "import keras\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn import metrics\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"C:\\\\Users\\\\Korisnik\\\\Downloads\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='crimson'>\n",
    "\n",
    "# 1. Preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and filling in missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Dataset-sa-generatorom222.csv')\n",
    "\n",
    "data=data.drop(1094,axis=0)                       #dropping the extra row which is empty\n",
    "data=data.drop('Maximum speed of wind', axis=1)   #dropping the column with no entries\n",
    "\n",
    "for i in range(1,data.shape[1]):\n",
    "    data.iloc[:,i] = pd.to_numeric(data.iloc[:,i], errors='coerce')    #transforming data from \"object\" to \"float\"\n",
    "    \n",
    "data.iloc[:,:-1] = data.iloc[:,:-1].fillna(method='ffill')  #filling in missing values from attribute columns \n",
    "\n",
    "data['Output']=data.iloc[:,-1].interpolate() #interpolating missing output data\n",
    "\n",
    "data['Day'] = pd.to_datetime(data['Day'],dayfirst=True)\n",
    "data['Year'] = data.Day.map(lambda x: x.year)\n",
    "data['Month'] = data.Day.map(lambda x: x.month)\n",
    "data['DayOfMonth'] = data.Day.map(lambda x: x.day) #separating date instances\n",
    "\n",
    "data = data.drop('Day', axis = 'columns')  \n",
    "\n",
    "cols=data.columns.tolist()\n",
    "cols = cols[-3:] + cols[:-3]\n",
    "data=data[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chencking if there are correlated columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data.iloc[:,3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= data.drop(['Maximum temperature','Minimum temperature', 'Maximum sustained wind speed'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since it is recommended that neural network input be a matrix with columns which are not highly correlated, we dropped the three columns which had high correlation with other columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting the data into seperate sets for crossvalidation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_2013   =  data[data['Year']!=2013]\n",
    "y_train_2013   =  x_train_2013['Output']\n",
    "x_train_2013   =  x_train_2013.iloc[:,3:-1]\n",
    "\n",
    "x_train_2014   =  data[data['Year']!=2014]\n",
    "y_train_2014   =  x_train_2014['Output']\n",
    "x_train_2014   =  x_train_2014.iloc[:,3:-1]\n",
    "\n",
    "x_train_2015   =  data[data['Year']!=2015]\n",
    "y_train_2015   =  x_train_2015['Output']\n",
    "x_train_2015   =  x_train_2015.iloc[:,3:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_2013    =  data[data['Year']==2013]\n",
    "y_test_2013    =  x_test_2013['Output']\n",
    "x_test_2013    =  x_test_2013.iloc[:,3:-1]\n",
    "\n",
    "x_test_2014    =  data[data['Year']==2014]\n",
    "y_test_2014    =  x_test_2014['Output']\n",
    "x_test_2014    =  x_test_2014.iloc[:,3:-1]\n",
    "\n",
    "x_test_2015    =  data[data['Year']==2015]\n",
    "y_test_2015    =  x_test_2015['Output']\n",
    "x_test_2015    =  x_test_2015.iloc[:,3:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Scaling input and output data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc= MinMaxScaler()\n",
    "\n",
    "x_train_2013   =  sc.fit_transform(x_train_2013)\n",
    "x_train_2014   =  sc.fit_transform(x_train_2014)\n",
    "x_train_2015   =  sc.fit_transform(x_train_2015)\n",
    "\n",
    "x_test_2013    =  sc.fit_transform(x_test_2013)\n",
    "x_test_2014    =  sc.fit_transform(x_test_2014)\n",
    "x_test_2015    =  sc.fit_transform(x_test_2015)\n",
    "\n",
    "y_train_2013   =  np.array(y_train_2013).reshape(-1,1)\n",
    "y_train_2013   =  sc.fit_transform(y_train_2013)\n",
    "y_train_2014   =  np.array(y_train_2014).reshape(-1,1)\n",
    "y_train_2014   =  sc.fit_transform(y_train_2014)\n",
    "y_train_2015   =  np.array(y_train_2015).reshape(-1,1)\n",
    "y_train_2015   =  sc.fit_transform(y_train_2015)\n",
    "\n",
    "y_test_2013    =  np.array(y_test_2013).reshape(-1,1)\n",
    "y_test_2013    =  sc.fit_transform(y_test_2013)\n",
    "y_test_2014    =  np.array(y_test_2014).reshape(-1,1)\n",
    "y_test_2014    =  sc.fit_transform(y_test_2014)\n",
    "y_test_2015    =  np.array(y_test_2015).reshape(-1,1)\n",
    "y_test_2015    =  sc.fit_transform(y_test_2015)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'crimslon'>\n",
    "\n",
    "# 2. Defining the Neural Network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return(1/(1+np.exp(-x)))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return(sigmoid(x)*(1-sigmoid(x)))\n",
    "\n",
    "def add_ones(a):                          #this function ads a column of ones as the first column of the input array\n",
    "    b=np.ones((a.shape[0],a.shape[1]+1))\n",
    "    b[:,1:]=a\n",
    "    return(b) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "    \n",
    "    def __init__(self, x, y):\n",
    "        self.input      = add_ones(x)\n",
    "        np.random.seed(2)\n",
    "        self.weights1   = np.random.rand(self.input.shape[1],15) \n",
    "        self.weights2   = np.random.rand(16,1) \n",
    "        self.y          = np.array(y)\n",
    "        self.output     = np.zeros(self.y.shape)\n",
    "        \n",
    "    def feedforward(self):\n",
    "        self.layer1 = sigmoid(np.dot(self.input, self.weights1))\n",
    "        self.output = sigmoid(np.dot(add_ones(self.layer1), self.weights2))\n",
    "        \n",
    "    def calculate_gradients(self):\n",
    "        self.d_weights2 = np.dot(add_ones(self.layer1).T, ((self.y - self.output) * sigmoid_derivative(self.output)))\n",
    "        self.d_weights1 = np.dot(self.input.T,  (np.dot((self.y - self.output) * sigmoid_derivative(self.output), self.weights2[1:].T) * sigmoid_derivative(self.layer1)))\n",
    "    \n",
    "    def backprop(self, mu=1):\n",
    "        self.calculate_gradients()\n",
    "        self.weights1+=mu*self.d_weights1\n",
    "        self.weights2+=mu*self.d_weights2  \n",
    "        \n",
    "    def backprop_corrected(self, mu, alpha=0.9):         #this function is used to speed up iteration procedure\n",
    "        d_weights2_old = self.d_weights2\n",
    "        d_weights1_old = self.d_weights1\n",
    "        \n",
    "        self.calculate_gradients()\n",
    "        \n",
    "        self.weights1 += alpha*(d_weights1_old) + (1-alpha)*mu*self.d_weights1\n",
    "        self.weights2 += alpha*(d_weights2_old) + (1-alpha)*mu*self.d_weights2\n",
    "                \n",
    "        \n",
    "    def train(self, mu=1 ,nr_it=10):\n",
    "        for i in range(nr_it):\n",
    "            self.feedforward()\n",
    "            self.backprop(mu)\n",
    "            \n",
    "            \n",
    "    def train_corrected(self, mu=1, alpha=0.9, nr_it=10):\n",
    "        self.feedforward()\n",
    "        self.backprop(mu)\n",
    "        for i in range(nr_it-1):\n",
    "            self.feedforward()\n",
    "            self.backprop_corrected(mu, alpha)\n",
    "            \n",
    "    def predict_y(self,x):\n",
    "        return(sigmoid(np.dot(add_ones(sigmoid(np.dot(add_ones(x),self.weights1))),self.weights2)))\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a small dataset and testing our algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1=np.array([[0,0,1],[0,1,1],[1,0,1],[1,1,1]])\n",
    "y1=np.array([[0],[1],[1],[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN1=NeuralNetwork(x1,y1)\n",
    "NN1.train(1, nr_it=10000)\n",
    "\n",
    "plt.figure(figsize=(12,7))\n",
    "plt.scatter(range(y1.size),y1)\n",
    "#plt.plot(range(trainingdata2013_output.size),trainingdata2013_output)\n",
    "plt.scatter(range(NN1.output.size), NN1.output)\n",
    "plt.legend(['original output', 'predicted output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blj = pd.DataFrame(NN1.output, columns=['prediction'])\n",
    "blj['y']=y1\n",
    "blj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can see that the algorithm works!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We now try to train three neural networks to predict data for three seperate years:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN2013 = NeuralNetwork(x_train_2013,y_train_2013)\n",
    "NN2013.train(mu=0.013, nr_it=1500)\n",
    "y_pred_2013 = NN2013.predict_y(x_test_2013)\n",
    "e2013 = metrics.mean_absolute_error(y_pred_2013, y_test_2013)\n",
    "\n",
    "NN2014 = NeuralNetwork(x_train_2014,y_train_2014)\n",
    "NN2014.train(mu=0.013, nr_it=1500)\n",
    "y_pred_2014 = NN2014.predict_y(x_test_2014)\n",
    "e2014 = metrics.mean_absolute_error(y_pred_2014, y_test_2014)\n",
    "\n",
    "NN2015 = NeuralNetwork(x_train_2015,y_train_2015)\n",
    "NN2015.train(mu=0.013, nr_it=1500)\n",
    "y_pred_2015 = NN2015.predict_y(x_test_2015)\n",
    "e2015 = metrics.mean_absolute_error(y_pred_2015, y_test_2015)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For reference, we use built-in functions from the Keras package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_regressor():\n",
    "    regressor = Sequential()\n",
    "    regressor.add(Dense(units=6, input_dim=6))\n",
    "    regressor.add(Dense(units=1))\n",
    "    regressor.compile(optimizer='sgd', loss='mean_squared_error',  metrics=['mae','accuracy'])\n",
    "    return regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN2013_keras = KerasRegressor(build_fn=build_regressor, batch_size=730,epochs=1500)\n",
    "NN2013_keras.fit(x_train_2013, y_train_2013)\n",
    "y_pred_2013_keras = NN2013_keras.predict(x_test_2013)\n",
    "e2013_keras = metrics.mean_absolute_error(y_pred_2013_keras, y_test_2013)\n",
    "\n",
    "\n",
    "NN2014_keras = KerasRegressor(build_fn=build_regressor, batch_size=730,epochs=1500)\n",
    "NN2014_keras.fit(x_train_2014, y_train_2014)\n",
    "y_pred_2014_keras= NN2014_keras.predict(x_test_2014)\n",
    "e2014_keras = metrics.mean_absolute_error(y_pred_2014_keras, y_test_2014)\n",
    "\n",
    "NN2015_keras = KerasRegressor(build_fn=build_regressor, batch_size=729,epochs=1500)\n",
    "NN2015_keras.fit(x_train_2015, y_train_2015)\n",
    "y_pred_2015_keras= NN2015_keras.predict(x_test_2015)\n",
    "e2015_keras = metrics.mean_absolute_error(y_pred_2015_keras, y_test_2015)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On the following plot we can se comparisons of real output, our predictions and keras predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,20))\n",
    "\n",
    "plt.subplot(3,1,1)\n",
    "plt.plot(range(y_test_2013.size),y_test_2013)\n",
    "plt.plot(range(y_pred_2013.size),y_pred_2013)\n",
    "plt.plot(y_pred_2013_keras)\n",
    "plt.legend(['real output', 'prediction our function', 'prediction built-in function'])\n",
    "plt.title('Prediction for year 2013')\n",
    "\n",
    "\n",
    "plt.subplot(3,1,2)\n",
    "plt.plot(range(y_test_2014.size),y_test_2014)\n",
    "plt.plot(range(y_pred_2014.size),y_pred_2014)\n",
    "plt.plot(y_pred_2014_keras)\n",
    "plt.legend(['real output', 'prediction our function', 'prediction built-in function'])\n",
    "plt.title('Prediction for year 2014')\n",
    "\n",
    "plt.subplot(3,1,3)\n",
    "plt.plot(range(y_test_2015.size),y_test_2015)\n",
    "plt.plot(range(y_pred_2015.size),y_pred_2015)\n",
    "plt.plot(y_pred_2015_keras)\n",
    "plt.legend(['real output', 'prediction of our function', 'prediction of built-in function'])\n",
    "plt.title('Prediction for year 2015')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  We can compare the performance of our NN against Keras NN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAE = [e2013, e2014, e2015]\n",
    "KERAS_MAE = [e2013_keras, e2014_keras, e2015_keras]\n",
    "\n",
    "ind = np.arange(3)  # the x locations for the groups\n",
    "width = 0.2\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (12,7))\n",
    "rects1 = ax.bar(ind - width/2, MAE, width, label='Our NN', color = 'pink')\n",
    "rects2 = ax.bar(ind + width/2, KERAS_MAE, width, label='Keras NN', color = 'magenta')\n",
    "\n",
    "ax.set_ylabel('Mean Absolute Error')\n",
    "ax.set_xlabel('Years')\n",
    "ax.set_title('Error of our NN vs. error of keras NN')\n",
    "ax.set_xticks(ind)\n",
    "ax.set_xticklabels(('2013', '2014', '2015'))\n",
    "ax.legend()\n",
    "plt.show()\n",
    "plt.savefig('mse', dpi = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='crimson'>\n",
    "\n",
    "# 4. Crossvalidation: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will use construct different models using different parameters for step size in gradient descent $\\mu$ and number of iterations, and then using crossvalidation method to estimate the error of our prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_iterations = [500,1000,2000,2500]\n",
    "step_sizes = [0.001, 0.005, 0.01, 0.015, 0.02, 0.1]\n",
    "errors=pd.DataFrame([], columns=nr_iterations, index = step_sizes)\n",
    "\n",
    "for i in nr_iterations:\n",
    "    for j in step_sizes:\n",
    "        NN2013 = NeuralNetwork(x_train_2013,y_train_2013)\n",
    "        NN2013.train(mu=j, nr_it=i)\n",
    "        y_pred_2013 = NN2013.predict_y(x_test_2013)\n",
    "        e2013 = metrics.mean_absolute_error(y_pred_2013, y_test_2013)\n",
    "\n",
    "        NN2014 = NeuralNetwork(x_train_2014,y_train_2014)\n",
    "        NN2014.train(mu=j, nr_it=i)\n",
    "        y_pred_2014 = NN2014.predict_y(x_test_2014)\n",
    "        e2014 = metrics.mean_absolute_error(y_pred_2014, y_test_2014)\n",
    "\n",
    "        NN2015 = NeuralNetwork(x_train_2015,y_train_2015)\n",
    "        NN2015.train(mu=j, nr_it=i)\n",
    "        y_pred_2015 = NN2015.predict_y(x_test_2015)\n",
    "        e2015 = metrics.mean_absolute_error(y_pred_2015, y_test_2015)\n",
    "\n",
    "        errors.loc[j,i] =  (e2013+e2014+e2015)/3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
